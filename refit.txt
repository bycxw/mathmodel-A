params = {
    'learning_rate': 0.15,
    'boosting_type': 'gbdt',
    'max_bin': 60,
    'objective': 'regression_l2',
    'metric': 'mse',
    'feature_fraction': 0.6,
    'bagging_fraction': 0.8,
    'bagging_freq': 10,
    'num_leaves': 500,
    'verbose': -1,
    'max_depth': -1,
    'reg_alpha':0.2,
    'reg_lambda':0.4,
    'nthread': 8
}
#降低chunk时要同时增大refit次数
chunk=900000
count=0
#先shuffle train_set
for i in range(1,14):
    #分14部分，留最后一部分当test_set
    train_i=train_set[(i-1)*chunk:i*chunk]
    train_x,train_y=train_i[features1],train_i['RSRP']
    if count==0:
        num_round = 2000
        trn_data = lgb.Dataset(train_x,label=train_y)
        clf = lgb.train(params, trn_data, num_round)
    else:
        decay_rate=1/count+1
        clf=clf.refit(train_x,train_y,decay_rate=decay_rate)
    count+=1    
